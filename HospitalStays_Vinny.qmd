---
title: Hospital Stay Models
author: 
  - name: Adam Gilbert
    email: a.gilbert1@snhu.edu
    affiliations: 
      - name: Southern New Hampshire University
format: html
date: 2/24/2025
date-modified: today
date-format: long
theme: flatly
toc: true
code-fold: true
---

```{r}
#| label: setup
#| message: false
#| warning: false

library(tidyverse)
library(tidymodels)

unregister <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}

data <- read_csv("https://raw.githubusercontent.com/agmath/agmath.github.io/master/data/classification/hospital_stays_small.csv")

names(data) <- janitor::make_clean_names(names(data))
```

```{r}
set.seed(559946)
hospital_split <- initial_split(data, strata = stay)

train <- training(hospital_split)
test <- testing(hospital_split)

train_folds <- vfold_cv(train, v = 10, strata = stay)
```

```{r}
knn_spec <- nearest_neighbor() |>
  set_engine("kknn") |>
  set_mode("classification")

knn_rec <- recipe(stay ~ ., data = train) |>
  step_normalize(all_numeric_predictors()) |>
  step_rm(all_nominal_predictors()) |>
  step_rm(case_id) |>
  step_impute_median(all_numeric_predictors())

knn_wf <- workflow() |>
  add_model(knn_spec) |>
  add_recipe(knn_rec)
```

```{r}
n_cores <- parallel::detectCores()
cl <- parallel::makeCluster(n_cores - 1, type = "PSOCK")
doParallel::registerDoParallel(cl)

tictoc::tic()

knn_cv_results <- knn_wf |>
  fit_resamples(
    resamples = train_folds,
    metrics = metric_set(accuracy, mn_log_loss)
  )

tictoc::toc()

doParallel::stopImplicitCluster()
unregister()

knn_cv_results |>
  collect_metrics()

```

</br>

### Hyperparameter Tuning with Parallel Processing

```{r}
#| label: about-7-min

n_cores <- parallel::detectCores()
cl <- parallel::makeCluster(n_cores - 1, type = "PSOCK")
doParallel::registerDoParallel(cl)

tictoc::tic()

knn_tune_results <- knn_tune_wf %>%
  tune_bayes(
    resamples = train_folds,
    metrics = metric_set(mn_log_loss),
    control = control_bayes(parallel_over = "everything")
  )

tictoc::toc()

doParallel::stopImplicitCluster()
unregister()

knn_cv_results %>%
  collect_metrics()
```

## Fit KNN with Optimal Neighbors (Best Hyperparameters)

```{r}
knn_best_params <- knn_tune_results %>%
  select_best(metric = "mn_log_loss")

knn_best_wf <- knn_tune_wf %>%
  finalize_workflow(knn_best_params)

knn_best_fit <- knn_best_wf %>%
  fit(train)
```

## Plotting Decision Boundary

```{r}
new_data <- crossing(
  case_id = NA,
  hospital_code = median(train$hospital_code, na.rm = TRUE),
  hospital_type_code = mode(train$hospital_type_code),
  city_code_hospital = median(train$city_code_hospital),
  hospital_region_code = mode(train$hospital_code),
  available_extra_rooms_in_hospital = seq(min(train$available_extra_rooms_in_hospital, na.rm = TRUE),
            max(train$available_extra_rooms_in_hospital, na.rm = TRUE), 
            length.out = 100),
  department = mode(train$department),
  ward_type = mode(train$ward_type),
  ward_facility_code = mode(train$ward_facility_code),
  bed_grade = median(train$bed_grade, na.rm = TRUE),
  patientid = median(train$patientid, na.rm = TRUE),
  city_code_patient = median(train$city_code_patient, na.rm = TRUE),
  type_of_admission = mode(train$type_of_admission),
  severity_of_illness = mode(train$severity_of_illness),
  visitors_with_patient = median(train$visitors_with_patient, na.rm = TRUE),
  age = mode(train$age),
  admission_deposit = seq(min(train$admission_deposit, na.rm = TRUE),
            max(train$admission_deposit, na.rm = TRUE), 
            length.out = 100),
)

knn_best_fit %>%
  augment(new_data) %>%
  ggplot() + 
  geom_point(aes(x = admission_deposit, y = available_extra_rooms_in_hospital, color = .pred_class), alpha = 0.2) + 
  guides(color = guide_legend(override.aes = list(alpha = 1)))
```



## Implementing Decision Tree

```{r}
dt_spec <- decision_tree(min_n = tune(), tree_depth = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")

dt_rec <- recipe(stay ~ ., data = train) |>
  step_impute_median(all_numeric_predictors()) |>
  step_impute_mode(all_nominal_predictors()) |>
  step_dummy(all_nominal_predictors())

dt_wf <- workflow() |>
  add_model(dt_spec) |>
  add_recipe(dt_rec)
```

## DT Tuning

```{r}
n_cores <- parallel::detectCores()
cl <- parallel::makeCluster(n_cores - 1, type = "PSOCK")
doParallel::registerDoParallel(cl)

tictoc::tic()

dt_tune_results <- dt_wf %>%
  tune_bayes(
    resamples = train_folds,
    metrics = metric_set(mn_log_loss),
    initial = 5,
    control = control_bayes(parallel_over = "everything")
  )

tictoc::toc()

doParallel::stopImplicitCluster()
unregister()

dt_tune_results %>%
  collect_metrics()
```

## Fitting DT Model

```{r}
dt_best_params <- dt_tune_results %>%
  select_best(metric = "mn_log_loss")

dt_best_wf <- dt_tune_wf %>%
  finalize_workflow(knn_best_params)

dt_best_fit <- dt_best_wf %>%
  fit(train)
```

